---

- name: Add OpenJDK-r repository
  apt_repository:
    repo: 'ppa:openjdk-r/ppa'
  when: ansible_distribution == 'Ubuntu' and ansible_lsb.major_release|int <= 15 and not skip_packages|default(False) and not use_oracle_java|default(False)

- block:
  - name: Add Oracle Java WebUpd8 repository
    apt_repository:
       repo: 'ppa:webupd8team/java'

  - name: Accept Oracle license
    debconf:
      name: "{{ java_package }} "
      question: "shared/accepted-oracle-license-v1-1"
      value: "true"
      vtype: "select"
  when: use_oracle_java| default(False) and not skip_packages|default(False)

- name: Select Java option to install (OpenJDK)
  include_vars:
      file: group_vars/all/openjdk.yml
  when: not use_oracle_java|default(False)

- name: Select Java option to install (Oracle)
  include_vars:
      file: group_vars/all/oracle.yml
  when: use_oracle_java|default(False)

- name: install python
  apt:
    name: python
    update_cache: yes

- name: update apt cache
  apt: update_cache=yes
  when: not skip_packages|default(False)
  retries: 2

- name: install packages
  package: name={{ item }} state=present
  with_items: "{{ ubuntu_packages }}"
  when: not skip_packages|default(False)

- name: install Java
  package: name={{ java_package }} state=present
  when: not skip_packages|default(False)



- name: disable net.ipv6.conf.all.disable_ipv6
  sysctl: name=net.ipv6.conf.all.disable_ipv6 value=1 state=present
  tags:
    - prepare

- name: disable net.ipv6.conf.lo.disable_ipv6
  sysctl: name=net.ipv6.conf.lo.disable_ipv6 value=1 state=present
  tags:
    - prepare

- name: increase hard file limits
  pam_limits: domain=* limit_type=hard limit_item=nofile value=1000000
  tags:
    - prepare

- name: increase soft file limits
  pam_limits: domain=* limit_type=soft limit_item=nofile value=1000000

- name: create hadoop group
  group: name=hadoop state=present
  tags:
    - prepare

- name: create hadoop user
  user: name={{ hadoop_user }} comment="Hadoop user" group=hadoop shell=/bin/bash
  tags:
    - prepare

- name: get Hadoop distro (except CDH4)
  become: no
  local_action: get_url url={{ hadoop_download_url }} dest=files/ checksum="md5:{{ hadoop_md5 }}"
  when: "hadoop_version != 'cdh4'"

- name: get Hadoop distro (CDH4)
  become: no
  local_action: get_url url={{ hadoop_download_url }} dest=files/
  when: "hadoop_version == 'cdh4'"


- name: distribute hadoop among slaves
  synchronize:
    src: "files/{{ hadoop_file }}.tar.gz"
    dest: "/usr/local/{{ hadoop_file }}.tar.gz"
    checksum: yes
#  delegate_to: "{{ active_master_inventory_hostname }}"

- name: unzip hadoop
  unarchive: copy=no src=/usr/local/{{ hadoop_file }}.tar.gz dest=/usr/local/ owner={{ hadoop_user }} group=hadoop

- name: create hadoop symlink
  file: src=/usr/local/{{ hadoop_file }} dest=/usr/local/hadoop state=link


- name: put needed swift library
  get_url: url={{ swift_download_url }} dest=/usr/local/{{ hadoop_file }}/share/hadoop/hdfs/lib/
  when: "hadoop_version == 2.3 or hadoop_version == 2.6"

#- name: remove hadoop archive
#  file: path=/usr/local/{{ hadoop_file }}.tar.gz state=absent

- name: set user and priviliges on hadoop
  file: path=/usr/local/{{ hadoop_file }} owner={{ hadoop_user }} group=hadoop recurse=yes

- name: get Spark distro
  become: no
  local_action: get_url url={{ spark_download_url }} dest=files/ checksum="md5:{{ spark_md5 }}"

#- name: put spark distro to master
#  copy: src={{ spark_arch }} dest=/opt/
#  when: "'{{ inventory_hostname }}' == '{{ active_master_inventory_hostname }}'"

- name: distribute spark distro among slaves
  synchronize:
    src: "files/{{ spark_file }}.tgz"
    dest: "/opt/{{ spark_file }}.tgz"
    checksum: yes
#  delegate_to: "{{ active_master_inventory_hostname }}"

- name: unzip spark
  unarchive: copy=no src=/opt/{{ spark_file }}.tgz dest=/opt

#- name: remove spark archive
#  file: path=/opt/{{ spark_file }}.tgz state=absent

- name: create spark symlink
  file: src={{ spark_home }} dest=/opt/spark state=link

- name: create spark symlink
  file: src={{ spark_home }} dest=/usr/local/spark state=link


- name: create extra jars directory
  file: path={{ spark_extra_jars_dir }} owner={{ hadoop_user }} group=hadoop state=directory

- name: copy extra jars
  copy: src={{item.path}} dest={{ spark_extra_jars_dir }}/{{item.name}} owner={{ hadoop_user }} group=hadoop mode=0644
  with_items: "{{extra_jars}}"

